{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\xef\\xbc\\xb5\\xef\\xbd\\x8e\\xef\\xbd\\x89\\xef\\xbd\\x83\\xef\\xbd\\x8f\\xef\\xbd\\x84\\xef\\xbd\\x85! \\xf0\\x9f\\x85\\xa4\\xf0\\x9f\\x85\\x9d\\xf0\\x9f\\x85\\x98\\xf0\\x9f\\x85\\x92\\xf0\\x9f\\x85\\x9e\\xf0\\x9f\\x85\\x93\\xf0\\x9f\\x85\\x94\\xe2\\x80\\xbd \\xf0\\x9f\\x87\\xba\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb3\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xae\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa8\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb4\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa9\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xaa! \\xf0\\x9f\\x98\\x84 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \\xe2\\x80\\x9csupport Unicode\\xe2\\x80\\x9d in our software (whatever that means\\xe2\\x80\\x94like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don\\xe2\\x80\\x99t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode\\xe2\\x80\\x99s inception.\\n'\n",
      "[10, 239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46, 10]\n",
      "len(text)=535\n",
      "len(tokens)=618\n"
     ]
    }
   ],
   "source": [
    "tokens = text.encode(\"utf-8\")\n",
    "print(tokens)\n",
    "tokens = list(map(int, tokens))   # list(tokens) also possible?\n",
    "print(tokens)\n",
    "\n",
    "print(f\"{len(text)=}\")\n",
    "print(f\"{len(tokens)=}\")\n",
    "\n",
    "# here we can see len(tokens) > len(text) because most of simple chars / ASCII chars become single byte but some complex chars become more than single bytes and goes upto 4 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xf0\\x9f\\x85\\x94' [240, 159, 133, 148]\n",
      "b'\\xe2\\x80\\x99' [226, 128, 153]\n",
      "b'\\xf0\\x9f\\x98\\x84' [240, 159, 152, 132]\n"
     ]
    }
   ],
   "source": [
    "tmp1 = \"ðŸ…”\".encode(\"utf-8\")\n",
    "tmp2 = \"â€™\".encode(\"utf-8\")\n",
    "tmp3 = \"ðŸ˜„\".encode(\"utf-8\")\n",
    "\n",
    "print(tmp1, list(tmp1))\n",
    "print(tmp2, list(tmp2))\n",
    "print(tmp3, list(tmp3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Most common pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (46, 10)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40)), (1, (10, 239))]\n",
      "(101, 32)\n"
     ]
    }
   ],
   "source": [
    "def get_most_common_pair(tokens_list):\n",
    "    counts = {}\n",
    "    for i,j in zip(tokens_list, tokens_list[1:]):\n",
    "        if (i,j) in counts:\n",
    "            counts[(i,j)] += 1\n",
    "        else:\n",
    "            counts[(i,j)] = 1\n",
    "\n",
    "    # alt    \n",
    "    # for pair in zip(tokens_list, tokens_list[1:]):\n",
    "    #     counts[pair] = counts.get(pair, 0) + 1\n",
    "\n",
    "    print(sorted([(v,k) for k,v in counts.items()], reverse=True))\n",
    "    \n",
    "    return counts, max(counts, key=counts.get)\n",
    "\n",
    "_ , most_common_pair = get_most_common_pair(tokens_list=tokens)\n",
    "print(most_common_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace most common pair with new index > 256 (earlier range=[0,255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [226]\n",
      "2 [226, 128]\n",
      "3 [226, 128, 107]\n",
      "5 [226, 128, 107, 256]\n",
      "6 [226, 128, 107, 256, 117]\n",
      "7 [226, 128, 107, 256, 117, 115]\n",
      "8 [226, 128, 107, 256, 117, 115, 105]\n",
      "9 [226, 128, 107, 256, 117, 115, 105, 110]\n",
      "10 [226, 128, 107, 256, 117, 115, 105, 110, 103]\n",
      "11 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32]\n",
      "12 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111]\n",
      "13 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101]\n",
      "14 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114]\n",
      "15 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32]\n",
      "16 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97]\n",
      "17 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108]\n",
      "18 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108]\n",
      "19 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32]\n",
      "20 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116]\n",
      "21 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104]\n",
      "23 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104, 256]\n",
      "24 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115]\n",
      "25 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116]\n",
      "26 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114]\n",
      "27 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105]\n",
      "28 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110]\n",
      "29 [226, 128, 107, 256, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[226,\n",
       " 128,\n",
       " 107,\n",
       " 256,\n",
       " 117,\n",
       " 115,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 32,\n",
       " 111,\n",
       " 101,\n",
       " 114,\n",
       " 32,\n",
       " 97,\n",
       " 108,\n",
       " 108,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 256,\n",
       " 115,\n",
       " 116,\n",
       " 114,\n",
       " 105,\n",
       " 110,\n",
       " 103]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge(tokens_list, pair, new_idx):\n",
    "    new_tokens_list = []\n",
    "\n",
    "    i=0\n",
    "    while i <= len(tokens_list)-1:\n",
    "        if tokens_list[i] == pair[0] and tokens_list[i+1] == pair[1]:\n",
    "            new_tokens_list.append(new_idx)\n",
    "            i += 1\n",
    "        else:\n",
    "            new_tokens_list.append(tokens_list[i])\n",
    "        \n",
    "        i += 1\n",
    "        print(i, new_tokens_list)\n",
    "    return new_tokens_list\n",
    "\n",
    "merge([226, 128, 107, 101, 32, 117, 115, 105, 110, 103, 32, 111, 101, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103],\n",
    "      (101, 32), \n",
    "      256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (46, 10)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40)), (1, (10, 239))]\n",
      "(101, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(618, 572)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, top_pair = get_most_common_pair(tokens)\n",
    "print(top_pair)\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "\n",
    "len(tokens), len(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together!!! BPE Implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens)=55913\n",
      "merged (101, 32) representing ('e', ' ') with occurence 1545 into new index 256\n",
      "merged (32, 116) representing (' ', 't') with occurence 1086 into new index 257\n",
      "merged (115, 32) representing ('s', ' ') with occurence 907 into new index 258\n",
      "merged (105, 110) representing ('i', 'n') with occurence 834 into new index 259\n",
      "merged (97, 110) representing ('a', 'n') with occurence 780 into new index 260\n",
      "merged (257, 104) representing ('Ä', 'h') with occurence 775 into new index 261\n",
      "merged (100, 32) representing ('d', ' ') with occurence 699 into new index 262\n",
      "merged (261, 256) representing ('Ä…', 'Ä€') with occurence 589 into new index 263\n",
      "merged (111, 110) representing ('o', 'n') with occurence 549 into new index 264\n",
      "merged (116, 32) representing ('t', ' ') with occurence 526 into new index 265\n",
      "merged (101, 114) representing ('e', 'r') with occurence 521 into new index 266\n",
      "merged (44, 32) representing (',', ' ') with occurence 508 into new index 267\n",
      "merged (114, 101) representing ('r', 'e') with occurence 435 into new index 268\n",
      "merged (111, 114) representing ('o', 'r') with occurence 403 into new index 269\n",
      "merged (105, 108) representing ('i', 'l') with occurence 372 into new index 270\n",
      "merged (101, 110) representing ('e', 'n') with occurence 363 into new index 271\n",
      "merged (105, 116) representing ('i', 't') with occurence 355 into new index 272\n",
      "merged (97, 108) representing ('a', 'l') with occurence 353 into new index 273\n",
      "merged (97, 116) representing ('a', 't') with occurence 346 into new index 274\n",
      "merged (46, 32) representing ('.', ' ') with occurence 329 into new index 275\n",
      "len(ids)=43638\n",
      "Compression: 1.2813\n"
     ]
    }
   ],
   "source": [
    "text = open(\"3iditos-wiki.txt\").read()\n",
    "\n",
    "tokens = text.encode(\"utf-8\")\n",
    "print(f\"{len(tokens)=}\")\n",
    "\n",
    "def get_most_common_pair(tokens_list):\n",
    "    counts = {}\n",
    "    for i,j in zip(tokens_list, tokens_list[1:]):\n",
    "        if (i,j) in counts:\n",
    "            counts[(i,j)] += 1\n",
    "        else:\n",
    "            counts[(i,j)] = 1\n",
    "\n",
    "    # alt    \n",
    "    # for pair in zip(tokens_list, tokens_list[1:]):\n",
    "    #     counts[pair] = counts.get(pair, 0) + 1\n",
    "\n",
    "    counts = sorted([(v,k) for k,v in counts.items()], reverse=True)\n",
    "    \n",
    "    return counts[0]\n",
    "\n",
    "def merge(tokens_list, pair, new_idx):\n",
    "    new_tokens_list = []\n",
    "\n",
    "    i=0\n",
    "    while i <= len(tokens_list)-1:\n",
    "        if tokens_list[i] == pair[0] and tokens_list[i+1] == pair[1]:\n",
    "            new_tokens_list.append(new_idx)\n",
    "            i += 1\n",
    "        else:\n",
    "            new_tokens_list.append(tokens_list[i])\n",
    "        \n",
    "        i += 1\n",
    "        # print(i, new_tokens_list)\n",
    "    return new_tokens_list\n",
    "\n",
    "\n",
    "# most_common_pair = get_most_common_pair(tokens_list=tokens)\n",
    "\n",
    "# ---------------------------\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "\n",
    "ids = list(tokens)  # copy\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    occur, most_common_pair = get_most_common_pair(ids)\n",
    "    idx = 256 + i\n",
    "    ids = merge(ids, most_common_pair, idx)\n",
    "    merges[most_common_pair] = idx\n",
    "    print(f\"merged {most_common_pair} representing {chr(most_common_pair[0]), chr(most_common_pair[1])} with occurence {occur} into new index {idx}\")  #todo bug chr(260)\n",
    "\n",
    "print(f\"{len(ids)=}\")\n",
    "print(f\"Compression: {(len(tokens)/len(ids)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can also see (10, 10) replaced with 272 > Giving a new token id for double whitespace line, similar thing was implemented from gpt2 to gpt4 progress tokenizer for writing python code. Giving a single token for 3 whitespaces, a single token for 7 whitespaces...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(0, 256)}\n",
    "for pair, idx in merges.items():\n",
    "    vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf=8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ï¿½'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=[108, 256, 88]\n",
      "le X\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def encode(text):\n",
    "    tokens = text.encode(\"utf-8\")\n",
    "    tokens = list(tokens)\n",
    "\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key= lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        tokens = merge(tokens, pair, merges[pair])\n",
    "    return tokens\n",
    "\n",
    "t = encode(\"le X\")\n",
    "print(f\"{t=}\")\n",
    "print(f\"{decode(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (32, 116): 257,\n",
       " (115, 32): 258,\n",
       " (105, 110): 259,\n",
       " (97, 110): 260,\n",
       " (257, 104): 261,\n",
       " (100, 32): 262,\n",
       " (261, 256): 263,\n",
       " (111, 110): 264,\n",
       " (116, 32): 265,\n",
       " (101, 114): 266,\n",
       " (44, 32): 267,\n",
       " (114, 101): 268,\n",
       " (111, 114): 269,\n",
       " (105, 108): 270,\n",
       " (101, 110): 271,\n",
       " (105, 116): 272,\n",
       " (97, 108): 273,\n",
       " (97, 116): 274,\n",
       " (46, 32): 275}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hfraf c8aocq n19238712 239& &%*()\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hfraf c8aocq n19238712 239& &%*()\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generally it will work, but there is always a corner case. Since not all tokens sequences are valid utf-8 byte chars and some of them cant be decodable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original GPT2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-30 06:45:13--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [application/octet-stream]\n",
      "Saving to: â€˜vocab.bpeâ€™\n",
      "\n",
      "vocab.bpe           100%[===================>] 445.62K   292KB/s    in 1.5s    \n",
      "\n",
      "2024-12-30 06:45:16 (292 KB/s) - â€˜vocab.bpeâ€™ saved [456318/456318]\n",
      "\n",
      "--2024-12-30 06:45:16--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: â€˜encoder.jsonâ€™\n",
      "\n",
      "encoder.json        100%[===================>]   1018K   451KB/s    in 2.3s    \n",
      "\n",
      "2024-12-30 06:45:19 (451 KB/s) - â€˜encoder.jsonâ€™ saved [1042301/1042301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From GPT2 Paper: \n",
    "We observed BPE including many versions of common words like dog\n",
    "since they occur in many variations such as dog. dog!\n",
    "dog? . This results in a sub-optimal allocation of limited\n",
    "vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any\n",
    "byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding\n",
    "only minimal fragmentation of words across multiple vocab\n",
    "tokens.\n",
    "\n",
    "Implements the RegexTokenizer that further splits the input text by a regex pattern, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Future:\n",
    "https://github.com/karpathy/minbpe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
